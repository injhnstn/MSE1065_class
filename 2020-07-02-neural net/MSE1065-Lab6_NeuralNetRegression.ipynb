{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Using neural networks to estimate Young's Modulus for elements*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages \n",
    "<ul>\n",
    "    <li>Pymatgen</li>\n",
    "    <li> Mendeleev</li>\n",
    "    <li> SKlearn </li>\n",
    "    <li> Matplotlib </li>\n",
    "    </ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will learn how to use neural networks from the SKlearn library to create a regression model to estimate Young's modulus.\n",
    "\n",
    "Agenda for this lab is \n",
    "\n",
    "1. Query online databases such as Pymatgen and Mendeleev to obtain element attributes and properties\n",
    "2. Use SKLearn's nonlinear module for Neural network regression\n",
    "3. Analyze the model performance by varying number of hidden units and hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Getting a dataset\n",
    "\n",
    "In this section we will query both [Pymatgen](http://pymatgen.org/) and [Mendeleev](https://mendeleev.readthedocs.io/en/stable/) to get a complete set of properties per element. We will use this data to create the cases from which the model will train and test.\n",
    "<br>\n",
    "<br>\n",
    "In this first snippet of code we will import all relevant libraries, the elements that will be turned into cases and the properties that will serve as the attributes for the cases. We will get 49 entries (which is a small dataset), but should give us a somewhat accurate prediction. We will also include some values to \"patch\" some unknown values in the dataset. It is important to note that more entries would move the prediction closer to the real value, and so would more attributes.\n",
    "<br>\n",
    "<br>\n",
    "The elements listed were chosen because querying them for these properties yields a dataset with few unknown values, and because they represent the three most common crystallographic structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymatgen as pymat\n",
    "import mendeleev as mendel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcc_elements = [\"Ag\", \"Al\", \"Au\", \"Cu\", \"Ir\", \"Ni\", \"Pb\", \"Pd\", \"Pt\", \"Rh\", \"Th\", \"Yb\"]\n",
    "bcc_elements = [\"Ba\", \"Cr\", \"Cs\", \"Eu\", \"Fe\", \"Li\", \"Mn\", \"Mo\", \"Na\", \"Nb\", \"Rb\", \"Ta\", \"V\", \"W\" ]\n",
    "hcp_elements = [\"Be\", \"Ca\", \"Cd\", \"Co\", \"Dy\", \"Er\", \"Gd\", \"Hf\", \"Ho\", \"Lu\", \"Mg\", \"Re\", \n",
    "                \"Ru\", \"Sc\", \"Tb\", \"Ti\", \"Tl\", \"Tm\", \"Y\", \"Zn\", \"Zr\"]\n",
    "others = [\"Si\", \"Ge\"] # \"Si\" and \"Ge\" are Face-centered diamond-cubic;\n",
    "\n",
    "\n",
    "# This is the list containing all the above elements\n",
    "elements = fcc_elements + others + bcc_elements + hcp_elements\n",
    "\n",
    "# Below is the list of features that we need to obtain from Mendeleev database\n",
    "querable_mendeleev = [\"atomic_number\", \"atomic_volume\", \"boiling_point\",\n",
    "                      \"en_ghosh\",  \"evaporation_heat\", \"heat_of_formation\",\n",
    "                     \"lattice_constant\", \"specific_heat\"]\n",
    "\n",
    "# Below is the list of features that we need to obtain from Pymaten database\n",
    "querable_pymatgen = [\"atomic_mass\", \"atomic_radius\", \"electrical_resistivity\",\n",
    "                     \"molar_volume\", \"bulk_modulus\", \"youngs_modulus\",\n",
    "                     \"average_ionic_radius\", \"density_of_solid\",\n",
    "                     \"coefficient_of_linear_thermal_expansion\"]\n",
    "\n",
    "#The list below includes all the properties\n",
    "querable_values = querable_mendeleev + querable_pymatgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting these values, we will proceed with our queries. Depending on the database (either Pymatgen or Mendeleev) where the property can be found, the code below fills up a list with the properties of each of the elements. To visualize how the dataset we just created looks, we will use the [Pandas](https://pandas.pydata.org/) library to display it. This library will take the list of lists and show it in a nice, user-friendly table with the properties as the column headers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue> **Exercise 1.** Fill in the code snippet below to query Pymatgen and Mendeleev for the required properties.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_values = [] # Values for Attributes\n",
    "all_labels = [] # Values for Young's Modulus (Property to be estimated)\n",
    "\n",
    "for item in elements:\n",
    "    element_values = []\n",
    "    \n",
    "    # This section queries Mendeleev\n",
    "    element_object = mendel.element(item) # Enter your code here\n",
    "    #Ques - Write a for loop over querable_mendeleev and append the property value in element_values\n",
    "    for i in querable_mendeleev:\n",
    "        element_values.append(getattr(element_object, i))\n",
    "    #df = df[querable_mendeleev]\n",
    "\n",
    "    # This section queries Pymatgen\n",
    "    element_object = pymat.Element(item) # Enter your code here\n",
    "    #Ques - Write a for loop over querable_pymatgen and append the property value in element_values\n",
    "    \n",
    "    for i in querable_pymatgen:\n",
    "        element_values.append(getattr(element_object, i))\n",
    "    \n",
    "    all_values.append(element_values) # All lists are appended to another list, creating a list of lists\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell once you are done with data query part in the cell above.\n",
    "# This cell is for debugging purpose. \n",
    "assert(len(all_values) == len(elements)),\"Len of all_values not equal to len of elements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code replaces some missing values in the data. You don't need to fill anything here. Just run this cell before moving forward.\n",
    "# Pandas Dataframe\n",
    "import os\n",
    "df = pd.DataFrame(all_values, columns=querable_values)\n",
    "\n",
    "# We will patch some of the values that are not available in the datasets.\n",
    "\n",
    "# Value for the CTE of Cesium\n",
    "index_Cs = df.index[df['atomic_number'] == 55]\n",
    "df.iloc[index_Cs, df.columns.get_loc(\"coefficient_of_linear_thermal_expansion\")] = 0.000097 \n",
    "# Value from: David R. Lide (ed), CRC Handbook of Chemistry and Physics, 84th Edition. CRC Press. Boca Raton, Florida, 2003\n",
    "\n",
    "# Value for the CTE of Rubidium\n",
    "index_Rb = df.index[df['atomic_number'] == 37]\n",
    "df.iloc[index_Rb, df.columns.get_loc(\"coefficient_of_linear_thermal_expansion\")] = 0.000090 \n",
    "# Value from: https://www.azom.com/article.aspx?ArticleID=1834\n",
    "\n",
    "# Value for the Evaporation Heat of Ruthenium\n",
    "index_Ru = df.index[df['atomic_number'] == 44]\n",
    "df.iloc[index_Ru, df.columns.get_loc(\"evaporation_heat\")] = 595 # kJ/mol \n",
    "# Value from: https://www.webelements.com/ruthenium/thermochemistry.html\n",
    "\n",
    "# Value for the Bulk Modulus of Zirconium\n",
    "index_Zr = df.index[df['atomic_number'] == 40]\n",
    "df.iloc[index_Zr, df.columns.get_loc(\"bulk_modulus\")] = 94 # GPa \n",
    "# Value from: https://materialsproject.org/materials/mp-131/\n",
    "\n",
    "# Value for the Bulk Modulus of Germanium\n",
    "index_Ge = df.index[df['atomic_number'] == 32]\n",
    "df.iloc[index_Ge, df.columns.get_loc(\"bulk_modulus\")] = 77.2 # GPa \n",
    "# Value from: https://www.crystran.co.uk/optical-materials/germanium-ge\n",
    "\n",
    "# Value for the Young's Modulus of Germanium\n",
    "index_Ge = df.index[df['atomic_number'] == 32]\n",
    "df.iloc[index_Ge, df.columns.get_loc(\"youngs_modulus\")] = 102.7 # GPa \n",
    "# Value from: https://www.crystran.co.uk/optical-materials/germanium-ge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atomic_number</th>\n",
       "      <th>atomic_volume</th>\n",
       "      <th>boiling_point</th>\n",
       "      <th>en_ghosh</th>\n",
       "      <th>evaporation_heat</th>\n",
       "      <th>heat_of_formation</th>\n",
       "      <th>lattice_constant</th>\n",
       "      <th>specific_heat</th>\n",
       "      <th>atomic_mass</th>\n",
       "      <th>atomic_radius</th>\n",
       "      <th>electrical_resistivity</th>\n",
       "      <th>molar_volume</th>\n",
       "      <th>bulk_modulus</th>\n",
       "      <th>average_ionic_radius</th>\n",
       "      <th>density_of_solid</th>\n",
       "      <th>coefficient_of_linear_thermal_expansion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>10.30</td>\n",
       "      <td>2485.0</td>\n",
       "      <td>0.147217</td>\n",
       "      <td>254.1</td>\n",
       "      <td>284.9</td>\n",
       "      <td>4.09</td>\n",
       "      <td>0.237</td>\n",
       "      <td>107.868200</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.630000e-08</td>\n",
       "      <td>10.27</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.086667</td>\n",
       "      <td>10490.0</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2740.0</td>\n",
       "      <td>0.150078</td>\n",
       "      <td>284.1</td>\n",
       "      <td>330.9</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.900</td>\n",
       "      <td>26.981539</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.700000e-08</td>\n",
       "      <td>10.00</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79</td>\n",
       "      <td>10.20</td>\n",
       "      <td>3080.0</td>\n",
       "      <td>0.261370</td>\n",
       "      <td>340.0</td>\n",
       "      <td>368.2</td>\n",
       "      <td>4.08</td>\n",
       "      <td>0.129</td>\n",
       "      <td>196.966569</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.200000e-08</td>\n",
       "      <td>10.21</td>\n",
       "      <td>220.0</td>\n",
       "      <td>1.070000</td>\n",
       "      <td>19300.0</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>7.10</td>\n",
       "      <td>2840.0</td>\n",
       "      <td>0.151172</td>\n",
       "      <td>304.6</td>\n",
       "      <td>337.4</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0.385</td>\n",
       "      <td>63.546000</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.720000e-08</td>\n",
       "      <td>7.11</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>8920.0</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77</td>\n",
       "      <td>8.54</td>\n",
       "      <td>4403.0</td>\n",
       "      <td>0.251060</td>\n",
       "      <td>604.0</td>\n",
       "      <td>669.0</td>\n",
       "      <td>3.84</td>\n",
       "      <td>0.133</td>\n",
       "      <td>192.217000</td>\n",
       "      <td>1.35</td>\n",
       "      <td>4.700000e-08</td>\n",
       "      <td>8.52</td>\n",
       "      <td>320.0</td>\n",
       "      <td>0.765000</td>\n",
       "      <td>22650.0</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28</td>\n",
       "      <td>6.60</td>\n",
       "      <td>3005.0</td>\n",
       "      <td>0.147207</td>\n",
       "      <td>378.6</td>\n",
       "      <td>430.1</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.443</td>\n",
       "      <td>58.693400</td>\n",
       "      <td>1.35</td>\n",
       "      <td>7.200000e-08</td>\n",
       "      <td>6.59</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>8908.0</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>82</td>\n",
       "      <td>18.30</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>0.177911</td>\n",
       "      <td>177.8</td>\n",
       "      <td>195.2</td>\n",
       "      <td>4.95</td>\n",
       "      <td>0.159</td>\n",
       "      <td>207.200000</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.100000e-07</td>\n",
       "      <td>18.26</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.122500</td>\n",
       "      <td>11340.0</td>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>46</td>\n",
       "      <td>8.90</td>\n",
       "      <td>3413.0</td>\n",
       "      <td>0.144028</td>\n",
       "      <td>372.4</td>\n",
       "      <td>376.6</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.244</td>\n",
       "      <td>106.420000</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.080000e-07</td>\n",
       "      <td>8.56</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.846250</td>\n",
       "      <td>12023.0</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>78</td>\n",
       "      <td>9.10</td>\n",
       "      <td>4100.0</td>\n",
       "      <td>0.256910</td>\n",
       "      <td>470.0</td>\n",
       "      <td>565.7</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.133</td>\n",
       "      <td>195.084000</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.060000e-07</td>\n",
       "      <td>9.09</td>\n",
       "      <td>230.0</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>21090.0</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>45</td>\n",
       "      <td>8.30</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.140838</td>\n",
       "      <td>494.0</td>\n",
       "      <td>556.0</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.244</td>\n",
       "      <td>102.905500</td>\n",
       "      <td>1.35</td>\n",
       "      <td>4.300000e-08</td>\n",
       "      <td>8.28</td>\n",
       "      <td>380.0</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>12450.0</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   atomic_number  atomic_volume  boiling_point  en_ghosh  evaporation_heat  \\\n",
       "0             47          10.30         2485.0  0.147217             254.1   \n",
       "1             13          10.00         2740.0  0.150078             284.1   \n",
       "2             79          10.20         3080.0  0.261370             340.0   \n",
       "3             29           7.10         2840.0  0.151172             304.6   \n",
       "4             77           8.54         4403.0  0.251060             604.0   \n",
       "5             28           6.60         3005.0  0.147207             378.6   \n",
       "6             82          18.30         2013.0  0.177911             177.8   \n",
       "7             46           8.90         3413.0  0.144028             372.4   \n",
       "8             78           9.10         4100.0  0.256910             470.0   \n",
       "9             45           8.30         4000.0  0.140838             494.0   \n",
       "\n",
       "   heat_of_formation  lattice_constant  specific_heat  atomic_mass  \\\n",
       "0              284.9              4.09          0.237   107.868200   \n",
       "1              330.9              4.05          0.900    26.981539   \n",
       "2              368.2              4.08          0.129   196.966569   \n",
       "3              337.4              3.61          0.385    63.546000   \n",
       "4              669.0              3.84          0.133   192.217000   \n",
       "5              430.1              3.52          0.443    58.693400   \n",
       "6              195.2              4.95          0.159   207.200000   \n",
       "7              376.6              3.89          0.244   106.420000   \n",
       "8              565.7              3.92          0.133   195.084000   \n",
       "9              556.0              3.80          0.244   102.905500   \n",
       "\n",
       "   atomic_radius  electrical_resistivity  molar_volume  bulk_modulus  \\\n",
       "0           1.60            1.630000e-08         10.27         100.0   \n",
       "1           1.25            2.700000e-08         10.00          76.0   \n",
       "2           1.35            2.200000e-08         10.21         220.0   \n",
       "3           1.35            1.720000e-08          7.11         140.0   \n",
       "4           1.35            4.700000e-08          8.52         320.0   \n",
       "5           1.35            7.200000e-08          6.59         180.0   \n",
       "6           1.80            2.100000e-07         18.26          46.0   \n",
       "7           1.40            1.080000e-07          8.56         180.0   \n",
       "8           1.35            1.060000e-07          9.09         230.0   \n",
       "9           1.35            4.300000e-08          8.28         380.0   \n",
       "\n",
       "   average_ionic_radius  density_of_solid  \\\n",
       "0              1.086667           10490.0   \n",
       "1              0.675000            2700.0   \n",
       "2              1.070000           19300.0   \n",
       "3              0.820000            8920.0   \n",
       "4              0.765000           22650.0   \n",
       "5              0.740000            8908.0   \n",
       "6              1.122500           11340.0   \n",
       "7              0.846250           12023.0   \n",
       "8              0.805000           21090.0   \n",
       "9              0.745000           12450.0   \n",
       "\n",
       "   coefficient_of_linear_thermal_expansion  \n",
       "0                                 0.000019  \n",
       "1                                 0.000023  \n",
       "2                                 0.000014  \n",
       "3                                 0.000017  \n",
       "4                                 0.000006  \n",
       "5                                 0.000013  \n",
       "6                                 0.000029  \n",
       "7                                 0.000012  \n",
       "8                                 0.000009  \n",
       "9                                 0.000008  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The labels (values for Young's modulus) are stored separately for clarity (We drop the column later)\n",
    "\n",
    "all_labels = df['youngs_modulus'].tolist()\n",
    "df = df.drop(['youngs_modulus'], axis=1)\n",
    "\n",
    "df.head(n=10) # With this line you can see the first ten entries of our database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue> **Exercise 2.** Use the Pandas dataframe created above to plot Young's modulus vs lattice constant. Note that this is recreating the plot from previous tutorials, but using the Pandas framework to slice and access data </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Youngs Modulus')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enter your code here\n",
    "\n",
    "# prep data\n",
    "x = df.lattice_constant.values\n",
    "y = all_labels\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "ax = plt.scatter(x, y)\n",
    "\n",
    "plt.xlabel('Lattice Constant')\n",
    "plt.ylabel('Youngs Modulus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Processing and Organizing Data\n",
    "\n",
    "Most machine learning models are trained on a subset of all the available data, called the \"training set\", and the models are tested on the remainder of the available data, called the \"testing set\". Model performance has often been found to be enhanced when the inputs are normalized.\n",
    "\n",
    "##### SETS\n",
    "\n",
    "With the dataset we just created, we have 49 entries for our model. We will train with 44 cases and test on the remaining 5 elements to estimate Young's Modulus.\n",
    "\n",
    "##### NORMALIZATION\n",
    "\n",
    "Each one of these input data features has different units and is represented in scales with distinct orders of magnitude. Datasets that contain inputs like this need to be normalized, so that quantities with large values do not *overwhelm* the neural network, forcing it tune its weights to account for the different scales of our input data. In this tutorial, we will use the Standard Score Normalization, which subtracts the mean of the feature and divide by its standard deviation.\n",
    "\n",
    "<span style=\"font-size:2em;\">$ \\frac{X - µ}{σ} $ </span>\n",
    "\n",
    "While our model might converge without feature normalization, the resultant model would be difficult to train and would be dependent on the choice of units used in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue> **Exercise 3.** Perform normalization on the input features stored in the dataframe df </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.25043608e-17 -1.35945676e-17 -3.17206578e-17  2.99080488e-16\n",
      " -2.31107650e-16  2.12981560e-16  3.17206578e-16  5.66440319e-17\n",
      "  6.91057189e-17  6.55371449e-16 -2.77555756e-17  6.96721592e-17\n",
      " -1.61435491e-16  1.57470409e-16  1.01959257e-17 -5.66440319e-17]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "meanX = df.values.mean(axis = 0)\n",
    "stdX =  df.values.std(axis = 0)\n",
    "normX =  (df.values - meanX) / stdX\n",
    "\n",
    "#check work\n",
    "print(normX.mean(axis = 0))\n",
    "print(normX.var(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue> **Exercise 4.** Check if there is any Nan in the dataframe normX </font> \n",
    "\n",
    "Hint - you may want to check numpy' isnan( ) method. This returns boolean matrix, the sum of which can tell the total number of NaN's in your input dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count how many NaN (aka True aka 1 values are in the original dataframe)\n",
    "\n",
    "np.count_nonzero(np.isnan(df.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split data into training and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue> **Exercise 5.** Use SKLearn's train_test_split to obtain training and test set. Use 10% as test set </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(normX, all_labels, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue> **Exercise 6.** Use SKLearn's neural network class to create a neural network with 20 hidden units and one layer. The default activation function is relu and we will use it for this part of the assignement. In the next part, we will change the activation function but for now lets keep everything except the number of hidden units at default values. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#define model\n",
    "\n",
    "reg = MLPRegressor(hidden_layer_sizes= (20, ), activation= 'relu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue> **Exercise 7.** Fit the above neural network on the training set. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(20, 1), learning_rate='constant',\n",
       "             learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Enter your code here\n",
    "\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue> **Exercise 8.** Use the fitted model to predict on training set and compute mean absolute error . </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean absolute error is 123.94\n"
     ]
    }
   ],
   "source": [
    "#load dependencies\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#get train predictions\n",
    "y_train_prediction = reg.predict(X_train)\n",
    "\n",
    "print('the mean absolute error for training data is %0.2f' % mean_absolute_error(y_train, y_train_prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue> **Exercise 9.** Use the fitted model to predict on test set and compute mean absolute error . </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean absolute error for testing data is 138.27\n"
     ]
    }
   ],
   "source": [
    "#load dependencies\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#get train predictions\n",
    "y_test_prediction = reg.predict(X_test)\n",
    "\n",
    "print('the mean absolute error for testing data is %0.2f' % mean_absolute_error(y_test, y_test_prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue> **Exercise 10.** In this part we will increase the number of hidden units to 100 and compare the performance to previous model. Fit a neural network with 100 hidden units to the above training data. Compute mean absolute error on both training and test set. Comment on your findings i.e. how MAE changed on increasing number of hidden units. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean absolute error for training data is 103.47\n",
      "the mean absolute error for testing data is 115.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "\n",
    "reg = MLPRegressor(hidden_layer_sizes= (100, ), \\\n",
    "                   activation= 'relu').fit(X_train, y_train)\n",
    "#get train predictions\n",
    "y_train_prediction = reg.predict(X_train)\n",
    "\n",
    "print('the mean absolute error for training data is %0.2f' % mean_absolute_error(y_train, y_train_prediction))\n",
    "\n",
    "#get train predictions\n",
    "y_test_prediction = reg.predict(X_test)\n",
    "\n",
    "print('the mean absolute error for testing data is %0.2f' % mean_absolute_error(y_test, y_test_prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue> **Exercise 11.** In the last part, we had a single layer of hidden units. In this part we will add an additional hidden layers of 100 units and compare the performance to previous model. Fit a neural network with 2 layers each with 100 hidden units to the above training data. Compute mean absolute error on both training and test set. Comment on your findings i.e. how MAE changed on increasing number of hidden layers. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean absolute error for training data is 42.32\n",
      "the mean absolute error for testing data is 47.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "\n",
    "reg = MLPRegressor(hidden_layer_sizes= (100, 100), \\\n",
    "                   activation= 'relu').fit(X_train, y_train)\n",
    "#get train predictions\n",
    "y_train_prediction = reg.predict(X_train)\n",
    "\n",
    "print('the mean absolute error for training data is %0.2f' % mean_absolute_error(y_train, y_train_prediction))\n",
    "\n",
    "#get train predictions\n",
    "y_test_prediction = reg.predict(X_test)\n",
    "\n",
    "print('the mean absolute error for testing data is %0.2f' % mean_absolute_error(y_test, y_test_prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on the above question\n",
    "\n",
    "Both the training and test error are lower with addition of more neurons and an extra layer. Increasing the number of neurons by 5 has a smaller affect on the MAE compared to adding extra layers. On the basis of inspection, increasing model complexity through extra layers improves its fit most rapidly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue> **Exercise 12.** So far, we have been working with Relu activation function. In this part we will change the activation function to be tanh and compare the performance to previous model. Fit a neural network with 2 layers each with 100 hidden units to the above training data, with tanh as activation function. Compute mean absolute error on both training and test set. Comment on your findings i.e. how MAE changed with activation function. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean absolute error for training data is 101.19\n",
      "the mean absolute error for testing data is 111.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "\n",
    "reg = MLPRegressor(hidden_layer_sizes= (100, 100), \\\n",
    "                   activation= 'tanh').fit(X_train, y_train)\n",
    "#get train predictions\n",
    "y_train_prediction = reg.predict(X_train)\n",
    "\n",
    "print('the mean absolute error for training data is %0.2f' % mean_absolute_error(y_train, y_train_prediction))\n",
    "\n",
    "#get train predictions\n",
    "y_test_prediction = reg.predict(X_test)\n",
    "\n",
    "print('the mean absolute error for testing data is %0.2f' % mean_absolute_error(y_test, y_test_prediction))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comment\n",
    "\n",
    "The 'rehu' function only activates the neuron if the value is greater than 0, so it is deactivating the features for all values less than the mean - giving a less complex data set to fit. Conversely, the hyperbolic tanget function does not deactivate any neurons completely, meaning the model has to fit all data values, giving an increased error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue> **Exercise 13.** A very important parameter during neural network training or for any optimization problem is the learning rate and maximum number of iterations. Sometimes, we get poor performance due to non-optimal setting of these hyperparameters. In this part of the lab, use the same neural network architecture as last cell. Change the maximum number of iterations to 1000 (default is 200). Fit a neural network with 2 layers each with 100 hidden units to the above training data, with tanh as activation function. Compute mean absolute error on both training and test set. Comment on your findings i.e. how MAE changed with maximum iterations. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean absolute error for training data is 59.25\n",
      "the mean absolute error for testing data is 76.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "\n",
    "reg = MLPRegressor(hidden_layer_sizes= (100, 100), \\\n",
    "                   activation= 'tanh', \\\n",
    "                  max_iter = 1000).fit(X_train, y_train)\n",
    "#get train predictions\n",
    "y_train_prediction = reg.predict(X_train)\n",
    "\n",
    "print('the mean absolute error for training data is %0.2f' % mean_absolute_error(y_train, y_train_prediction))\n",
    "\n",
    "#get train predictions\n",
    "y_test_prediction = reg.predict(X_test)\n",
    "\n",
    "print('the mean absolute error for testing data is %0.2f' % mean_absolute_error(y_test, y_test_prediction))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comment\n",
    "\n",
    "Increasing the max number of itterations gives the model more opportunities to fit the data, likely improving the fit. This is seen in the lower MAE values compared to the equivalent model with only 200 itterations. The cost of this is computational time, but in this case it is negligable given the small data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=blue> **Exercise 14.** We used SKLearn 's neural network class to fit five different neural networks. In this part, go through the documentation and change different parameters and report the lowest MAE on test set you achieved on this dataset. You can also monitor n_iter_ output of the neural network and see which optimization algorithm resulted in lowest number of iterations.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity constant 100\n",
      "the mean absolute error for training data is 92.89\n",
      "the mean absolute error for testing data is 84.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity constant 1000\n",
      "the mean absolute error for training data is 46.52\n",
      "the mean absolute error for testing data is 46.50\n",
      "identity constant 10000\n",
      "the mean absolute error for training data is 46.02\n",
      "the mean absolute error for testing data is 50.00\n",
      "identity invscaling 100\n",
      "the mean absolute error for training data is 91.85\n",
      "the mean absolute error for testing data is 81.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity invscaling 1000\n",
      "the mean absolute error for training data is 46.87\n",
      "the mean absolute error for testing data is 45.03\n",
      "identity invscaling 10000\n",
      "the mean absolute error for training data is 46.05\n",
      "the mean absolute error for testing data is 49.89\n",
      "identity adaptive 100\n",
      "the mean absolute error for training data is 90.78\n",
      "the mean absolute error for testing data is 82.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity adaptive 1000\n",
      "the mean absolute error for training data is 46.80\n",
      "the mean absolute error for testing data is 45.07\n",
      "identity adaptive 10000\n",
      "the mean absolute error for training data is 45.96\n",
      "the mean absolute error for testing data is 50.04\n",
      "logistic constant 100\n",
      "the mean absolute error for training data is 116.98\n",
      "the mean absolute error for testing data is 130.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic constant 1000\n",
      "the mean absolute error for training data is 75.52\n",
      "the mean absolute error for testing data is 81.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic constant 10000\n",
      "the mean absolute error for training data is 4.09\n",
      "the mean absolute error for testing data is 40.63\n",
      "logistic invscaling 100\n",
      "the mean absolute error for training data is 117.26\n",
      "the mean absolute error for testing data is 130.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic invscaling 1000\n",
      "the mean absolute error for training data is 77.18\n",
      "the mean absolute error for testing data is 81.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic invscaling 10000\n",
      "the mean absolute error for training data is 19.09\n",
      "the mean absolute error for testing data is 51.81\n",
      "logistic adaptive 100\n",
      "the mean absolute error for training data is 115.65\n",
      "the mean absolute error for testing data is 128.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic adaptive 1000\n",
      "the mean absolute error for training data is 75.86\n",
      "the mean absolute error for testing data is 81.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic adaptive 10000\n",
      "the mean absolute error for training data is 6.69\n",
      "the mean absolute error for testing data is 74.36\n",
      "tanh constant 100\n",
      "the mean absolute error for training data is 113.89\n",
      "the mean absolute error for testing data is 124.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh constant 1000\n",
      "the mean absolute error for training data is 59.12\n",
      "the mean absolute error for testing data is 77.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh constant 10000\n",
      "the mean absolute error for training data is 0.36\n",
      "the mean absolute error for testing data is 39.76\n",
      "tanh invscaling 100\n",
      "the mean absolute error for training data is 113.22\n",
      "the mean absolute error for testing data is 122.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh invscaling 1000\n",
      "the mean absolute error for training data is 59.41\n",
      "the mean absolute error for testing data is 77.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh invscaling 10000\n",
      "the mean absolute error for training data is 0.37\n",
      "the mean absolute error for testing data is 49.72\n",
      "tanh adaptive 100\n",
      "the mean absolute error for training data is 114.26\n",
      "the mean absolute error for testing data is 125.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh adaptive 1000\n",
      "the mean absolute error for training data is 59.19\n",
      "the mean absolute error for testing data is 76.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh adaptive 10000\n",
      "the mean absolute error for training data is 0.82\n",
      "the mean absolute error for testing data is 64.21\n",
      "relu constant 100\n",
      "the mean absolute error for training data is 59.84\n",
      "the mean absolute error for testing data is 57.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu constant 1000\n",
      "the mean absolute error for training data is 10.34\n",
      "the mean absolute error for testing data is 46.75\n",
      "relu constant 10000\n",
      "the mean absolute error for training data is 0.05\n",
      "the mean absolute error for testing data is 32.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu invscaling 100\n",
      "the mean absolute error for training data is 59.78\n",
      "the mean absolute error for testing data is 55.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu invscaling 1000\n",
      "the mean absolute error for training data is 10.66\n",
      "the mean absolute error for testing data is 39.21\n",
      "relu invscaling 10000\n",
      "the mean absolute error for training data is 0.06\n",
      "the mean absolute error for testing data is 54.77\n",
      "relu adaptive 100\n",
      "the mean absolute error for training data is 50.84\n",
      "the mean absolute error for testing data is 48.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu adaptive 1000\n",
      "the mean absolute error for training data is 13.62\n",
      "the mean absolute error for testing data is 42.86\n",
      "relu adaptive 10000\n",
      "the mean absolute error for training data is 0.06\n",
      "the mean absolute error for testing data is 57.63\n"
     ]
    }
   ],
   "source": [
    "activation_funcs = ['identity', 'logistic', 'tanh', 'relu']\n",
    "learning_rates = ['constant', 'invscaling', 'adaptive']\n",
    "max_iters = [100, 1000, 10000]\n",
    "\n",
    "for funcs in activation_funcs:\n",
    "    for rates in learning_rates:\n",
    "        for n in max_iters:\n",
    "\n",
    "            #define model\n",
    "            reg = MLPRegressor(hidden_layer_sizes= (100, 100), \\\n",
    "                               activation= funcs, \\\n",
    "                              max_iter = n, \\\n",
    "                              learning_rate= rates).fit(X_train, y_train)\n",
    "            #get train predictions\n",
    "            y_train_prediction = reg.predict(X_train)\n",
    "            y_test_prediction = reg.predict(X_test)\n",
    "\n",
    "            print(funcs, rates, n)\n",
    "            \n",
    "            print('the mean absolute error for training data is %0.2f' % mean_absolute_error(y_train, y_train_prediction))\n",
    "            #get train predictions\n",
    "            print('the mean absolute error for testing data is %0.2f' % mean_absolute_error(y_test, y_test_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
