{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 4 - Regression using linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this lab is to learn and implement various Linear regression models. The models will predict the fatigue of steel given various process parameters. The data is stored in the csv file provided and cell below read it in a Pandas dataframe.\n",
    "\n",
    "The lab is worth 30 marks plus additional 2 marks for bonus questions. Instructions for each part is provided in the corresponding cells. Please contact the TA if any instruction is not clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages required\n",
    "\n",
    "<ol>\n",
    "    <li> Numpy </li>\n",
    "    <li> Matplotlib </li>\n",
    "    <li> Pandas </li>\n",
    "    <li> SKLearn </li>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Loading data here \n",
    "data_file = \"40192_2013_16_MOESM1_ESM.csv\"\n",
    "df_loaded = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its always good idea to explore the data that is given to us. The dataset includes chemical compositions and various process parameters. Using these as inputs, we will try to predict fatigue in the steel samples. More details can be found at A.Agrawal et.al. (2014), \"Exploration of data science techniques to predict fatigue strength of steel from composition and processing parameters\", IMMI\n",
    "\n",
    "Ques 4.1 (1.5 points) Get the mean, minimum and maximum of each column of the pandas data frame 'df_loaded'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sl. No.</th>\n",
       "      <th>NT</th>\n",
       "      <th>THT</th>\n",
       "      <th>THt</th>\n",
       "      <th>THQCr</th>\n",
       "      <th>CT</th>\n",
       "      <th>Ct</th>\n",
       "      <th>DT</th>\n",
       "      <th>Dt</th>\n",
       "      <th>QmT</th>\n",
       "      <th>...</th>\n",
       "      <th>S</th>\n",
       "      <th>Ni</th>\n",
       "      <th>Cr</th>\n",
       "      <th>Cu</th>\n",
       "      <th>Mo</th>\n",
       "      <th>RedRatio</th>\n",
       "      <th>dA</th>\n",
       "      <th>dB</th>\n",
       "      <th>dC</th>\n",
       "      <th>Fatigue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>437.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>219.000000</td>\n",
       "      <td>872.299771</td>\n",
       "      <td>737.643021</td>\n",
       "      <td>25.949657</td>\n",
       "      <td>10.654462</td>\n",
       "      <td>128.855835</td>\n",
       "      <td>40.502059</td>\n",
       "      <td>123.699844</td>\n",
       "      <td>4.843936</td>\n",
       "      <td>35.491991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014611</td>\n",
       "      <td>0.517048</td>\n",
       "      <td>0.570458</td>\n",
       "      <td>0.067780</td>\n",
       "      <td>0.069794</td>\n",
       "      <td>923.629291</td>\n",
       "      <td>0.047181</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.007712</td>\n",
       "      <td>552.903890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>126.295289</td>\n",
       "      <td>26.212073</td>\n",
       "      <td>280.036541</td>\n",
       "      <td>10.263824</td>\n",
       "      <td>7.841437</td>\n",
       "      <td>281.743539</td>\n",
       "      <td>126.924697</td>\n",
       "      <td>267.128933</td>\n",
       "      <td>15.700076</td>\n",
       "      <td>19.419277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.852976</td>\n",
       "      <td>0.411769</td>\n",
       "      <td>0.049161</td>\n",
       "      <td>0.088124</td>\n",
       "      <td>576.617020</td>\n",
       "      <td>0.031093</td>\n",
       "      <td>0.008240</td>\n",
       "      <td>0.010418</td>\n",
       "      <td>186.630528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>825.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>225.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>110.000000</td>\n",
       "      <td>865.000000</td>\n",
       "      <td>845.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>590.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>448.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>219.000000</td>\n",
       "      <td>870.000000</td>\n",
       "      <td>845.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>505.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>328.000000</td>\n",
       "      <td>870.000000</td>\n",
       "      <td>855.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>1228.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>578.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>437.000000</td>\n",
       "      <td>930.000000</td>\n",
       "      <td>865.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>930.000000</td>\n",
       "      <td>540.000000</td>\n",
       "      <td>903.333000</td>\n",
       "      <td>70.200000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>5530.000000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>1190.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Sl. No.          NT         THT         THt       THQCr          CT  \\\n",
       "count  437.000000  437.000000  437.000000  437.000000  437.000000  437.000000   \n",
       "mean   219.000000  872.299771  737.643021   25.949657   10.654462  128.855835   \n",
       "std    126.295289   26.212073  280.036541   10.263824    7.841437  281.743539   \n",
       "min      1.000000  825.000000   30.000000    0.000000    0.000000   30.000000   \n",
       "25%    110.000000  865.000000  845.000000   30.000000    8.000000   30.000000   \n",
       "50%    219.000000  870.000000  845.000000   30.000000    8.000000   30.000000   \n",
       "75%    328.000000  870.000000  855.000000   30.000000    8.000000   30.000000   \n",
       "max    437.000000  930.000000  865.000000   30.000000   24.000000  930.000000   \n",
       "\n",
       "               Ct          DT          Dt         QmT  ...           S  \\\n",
       "count  437.000000  437.000000  437.000000  437.000000  ...  437.000000   \n",
       "mean    40.502059  123.699844    4.843936   35.491991  ...    0.014611   \n",
       "std    126.924697  267.128933   15.700076   19.419277  ...    0.006145   \n",
       "min      0.000000   30.000000    0.000000   30.000000  ...    0.003000   \n",
       "25%      0.000000   30.000000    0.000000   30.000000  ...    0.010000   \n",
       "50%      0.000000   30.000000    0.000000   30.000000  ...    0.015000   \n",
       "75%      0.000000   30.000000    0.000000   30.000000  ...    0.019000   \n",
       "max    540.000000  903.333000   70.200000  140.000000  ...    0.030000   \n",
       "\n",
       "               Ni          Cr          Cu          Mo     RedRatio  \\\n",
       "count  437.000000  437.000000  437.000000  437.000000   437.000000   \n",
       "mean     0.517048    0.570458    0.067780    0.069794   923.629291   \n",
       "std      0.852976    0.411769    0.049161    0.088124   576.617020   \n",
       "min      0.010000    0.010000    0.010000    0.000000   240.000000   \n",
       "25%      0.020000    0.120000    0.020000    0.000000   590.000000   \n",
       "50%      0.060000    0.710000    0.060000    0.000000   740.000000   \n",
       "75%      0.460000    0.980000    0.100000    0.170000  1228.000000   \n",
       "max      2.780000    1.170000    0.260000    0.240000  5530.000000   \n",
       "\n",
       "               dA          dB          dC      Fatigue  \n",
       "count  437.000000  437.000000  437.000000   437.000000  \n",
       "mean     0.047181    0.003391    0.007712   552.903890  \n",
       "std      0.031093    0.008240    0.010418   186.630528  \n",
       "min      0.000000    0.000000    0.000000   225.000000  \n",
       "25%      0.020000    0.000000    0.000000   448.000000  \n",
       "50%      0.040000    0.000000    0.000000   505.000000  \n",
       "75%      0.070000    0.000000    0.010000   578.000000  \n",
       "max      0.130000    0.050000    0.058000  1190.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enter your code here.\n",
    "\n",
    "df_loaded.describe()  #use built in functions from Pandas package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful function is to find unique element in a data frame.\n",
    "\n",
    "Ques 4.2 (0.5 point) Print the unique elements in the column labeled 'THT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30, 865, 845, 825, 855], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enter your code here\n",
    "\n",
    "df_loaded.THT.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we are seperating inputs and output (fatigue) from the dataframe. Run the cell before before you move forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_loaded.drop(['Fatigue'],axis=1)\n",
    "Y = df_loaded['Fatigue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each feature (columns of X) is having different distribution i.e. different mean, minimum and maximum. Before applying any machine learning algorithm, its important to normalize each features. Like the last lab, we will compute mean and standard deviation of each feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 4.3 (3 points) Preprocess the features to have unit normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1.] [ 0.00000000e+00 -2.04870446e-15 -2.11374269e-16  8.12977959e-17\n",
      "  0.00000000e+00 -1.30076473e-16 -9.75573550e-17  0.00000000e+00\n",
      "  0.00000000e+00  9.75573550e-17 -9.75573550e-17  6.50382367e-17\n",
      " -1.78855151e-16 -5.52825012e-16  1.13816914e-16 -2.60152947e-16\n",
      "  1.78855151e-16  1.62595592e-16 -3.25191183e-17  1.05687135e-16\n",
      " -3.08931624e-16  9.75573550e-17  6.50382367e-17 -9.75573550e-17\n",
      " -1.62595592e-17 -3.65840081e-17]\n"
     ]
    }
   ],
   "source": [
    "test = (X.values - X.values.mean(axis = 0))/(X.values.std(axis = 0))\n",
    "\n",
    "\n",
    "#make sure that the data columns have a mean of 0 and unit variance\n",
    "print(test.var(axis=0), test.mean(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 4.4 (1 point) Split the data into training and test set. Retain 80% of the data as training set and rest 20% as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(test, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 4.5 (2 points) Fit a linear regression model on the training set. Use SKlearn's Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dependencies\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#define model\n",
    "reg = LinearRegression()\n",
    "\n",
    "#Fit the data to the model\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 4.6 (2 points) Compute predictions on training set. Print the mean absolute error between predictions and true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error base on training dataset is : 19.926\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = reg.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print('Mean absolute error base on training dataset is : %0.2f' %mean_absolute_error(y_train, y_train_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 4.7 (2 points) Compute predictions on test set and report mean absolute error on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error base on training dataset is : 33.43\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = reg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print('Mean absolute error base on training dataset is : %0.2f' %mean_absolute_error(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 4.8 (1 point) Plot the test set predictions versus corresponding true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Y Predicted Values')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEGCAYAAAB2EqL0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de5SdVZnn8e8vlSJUEKyggSYVMNGmwwIzkFA6aLodLu1ERUlMgzCj3ahMM/YwreJakaRRkVmDiR0bL91LbbRbUREIGEO8EZCgzDhcpkKCIUCG2OGSCkpUIghlUkme+eN9T3JS9Z5Tb1Wd+/l91qp1ztnnPfXunZWcJ3s/+6KIwMzMbCQT6l0BMzNrDg4YZmaWiwOGmZnl4oBhZma5OGCYmVkuE+tdgWp55StfGTNmzKh3NczMmsr69et/HRFTs95r2YAxY8YM+vr66l0NM7OmIunJUu95SMrMzHJxwDAzs1wcMMzMLBcHDDMzy8UBw8zMcmnZWVJmZo1q9YZ+Vqzdwo5dA0zr7mLx/FksnNNT72qNyAHDzKyGVm/oZ+mqTQwM7gOgf9cAS1dtAmj4oOEhKTOzGlqxdsuBYFEwMLiPFWu31KlG+TlgmJnV0I5dA6MqbyQekjIzK6EauYZp3V30ZwSHad1d4/q9teAehplZhkKuoX/XAMHBXMPqDf3j+r2L58+iq7PjkLKuzg4Wz581rt9bCw4YZmYZqpVrWDinh2WLZtPT3YWAnu4uli2a3fAJb/CQlJlZpmrmGhbO6WmKADGUexhmZhlK5RSaIddQLQ4YZmYZmjnXUC0ekjIzy1AYMmrGFdnV4oBhZlZCs+YaqsVDUmZmlot7GGZmLaAWGxo6YJiZNblabWjoISkzsyZXqw0NHTDMzJpcrTY0dMAwM2tytVpk6IBhZtbkarXI0ElvM7MmV6tFhg4YZmYtoBaLDD0kZWZmubiHYWbWQGqxAG+sHDDMzBpErRbgjZWHpMzMGkStFuCNlQOGmVmDqNUCvLHykJSZWQVUIvcwrbuL/ozg0Cin/FWthyHpXyU9K+nhorKjJd0p6fH0cUrRe0slbZW0RdL8ovLTJW1K3/uCJFWrzmbWOlZv6Gfe8nXMXPID5i1fx+oN/VW919JVm+jfNUBwMPcw2ns2+il/1RyS+jrwliFlS4C7IuJE4K70NZJOBi4CTkk/80VJhT+1LwGXAiemP0N/p5nZIfJ8gVcyoFQq97BwTg/LFs2mp7sLAT3dXSxbNLshEt5QxSGpiLhH0owhxQuAM9Pn1wM/Aa5Iy2+KiN3ANklbgddLegI4KiLuBZD0DWAh8KNq1dvMRqcRp4GW+wJfOKen4rORKpl7aORT/mqd9D42Ip4BSB+PSct7gKeLrtuelvWkz4eWZ5J0qaQ+SX07d+6saMXNbLhKDcVU2khf4JWejVSrzf/qrVFmSWXlJaJMeaaIuC4ieiOid+rUqRWrnJlla9RpoCN9gVd6NlKj5x4qpdYB41eSjgNIH59Ny7cDxxddNx3YkZZPzyg3swbQqNNAR/oCr0aP4PDOg1+n3V2dDZV7qJRaB4w1wMXp84uB24rKL5I0SdJMkuT2A+mw1QuSzkhnR/1V0WfMrM4adSimkDzu7uo8UFb8hV7JHkFhWO65lwYPlO3eu38MtW581ZxWeyNwLzBL0nZJlwDLgTdLehx4c/qaiNgMrAQeAW4HLouIQj/3b4CvAluBX+CEt1nDaPShmOIv7udeGjyQX6nkbKRGHZarBkWUTAk0td7e3ujr66t3NcxaXiPOkgKYt3xd5iK4nu4ufrbk7IrdZ+aSH2QmVgVsW35uxe5TK5LWR0Rv1nte6W1m49Ko00BL5VGygsh4NPrq7EpqlFlSZtZGarEKu9QXttL7V0qjD8tVkgOGmdVUrdZuLJ4/q+S8/ErmFxp9dXYleUjKzGpqpFXYlbJwTg8fvnlj5nuVmvY7NH/z2QtPa8lAUeAehpnVVC3XbvRUcdpvo65yryYHDDOruHI5ilqu3ahmfqGdptMWOGCYWUWN9D/vWiaJq5lfaNRV7tXkHIaZVdRIOYrCl3Wt1m5Ua9pvO02nLXDAMLOKyvM/70ZduzEai+fPYvGtDzG47+Cyvc4OteR02gIHDDOrqGb6n/e4V6kPXeLdmhtnHOAchplVVLMsZBvvLKcVa7cwuP/QCDG4P1o66e0ehplVVDVyFNXYr2q860Gc9DYzq4BK5igqfZxqwXi/8Jtp6K1SPCRlZg2tWusdxrsepFmG3irJAcPMGlq1hn7G+4XfTntIFXhIyswaWvfkzkNOsysY79BPJXItrTA9eDQcMMysYa3e0M/v/7B3WHml1ju02xf+eHlIyswaVtbUVYAjDpvoL/o6cMAws4ZVKk/xu4HhQ1RWfQ4YZtawarmzrY3MAcPMGlY7Tl1tZE56m1nDqvXOtlbeiAFD0jxgY0S8KOk9wFzg8xHxZNVrZ2ZtzzOZGkeeIakvAS9JOhX4KPAk8I2q1srMzBpOnoCxNyICWEDSs/g8cGR1q2VmZo0mTw7jBUlLgb8E/kxSB9BZ3WqZmVmjydPDuBDYDbw/In4J9AArqlorMzNrOCMGjDRIfAeYlBb9GvhuNStlZmaNJ88sqb8GLgWOBl5D0sP4MnBOdatm1pyqcdiPWSPIMyR1GTAPeB4gIh4Hjqlmpcya1XiP/TRrZHkCxu6I2FN4IWkiLX/UudnYVOuwH7NGkCdg/FTS3wFdkt4M3AJ8r7rVMmtO7XjOs7WPPAFjCbAT2AT8V+CHwMeqWSmzZuXN8qyV5ZkltT8ivhIRF0TE+elzD0mZZfBmedbK8syS2kZGziIiXl2VGpk1MW+WZ60sz0rv3qLnhwMXkEyxNbMM3izPWlWeIanfFP30R8TngLNrUDczM2sgIwYMSXOLfnolfYBxbj4o6XJJmyU9LOlGSYdLOlrSnZIeTx+nFF2/VNJWSVskzR/Pvc3MbGzyDEn9Q9HzvcATwLvGekNJPcAHgZMjYkDSSuAi4GTgrohYLmkJyeysKySdnL5/CjAN+LGkP4mIfSVuYWZmVTBiwIiIs6p03y5Jg8BkYAewFDgzff964CfAFSTbqt8UEbuBbZK2Aq8H7q1CvczMrISSAUPSR8p9MCKuHcsNI6Jf0meAp4AB4I6IuEPSsRHxTHrNM5IK24/0APcV/YrtaVlWnS8l2feKE044YSzVM2t43qvK6qVcDuPIEX7GJM1NLABmkgwxHZEe/VryIxllmetAIuK6iOiNiN6pU6eOtYpmDct7VVk9lexhRMTVVbrnnwPbImIngKRVwBuBX0k6Lu1dHAc8m16/HTi+6PPTSYawzNpOub2q3MuwasuzcO9w4BKSpPPhhfKIeP8Y7/kUcIakySRDUucAfcCLwMXA8vTxtvT6NcC3JV1L0iM5EXhgjPc2q5h6DA15ryqrpzx7SX0T+CNgPvBTkv/hvzDWG0bE/cCtwIMk+1NNAK4jCRRvlvQ48Ob0NRGxGVgJPALcDlzmGVJWb/UaGvJeVVZPeQLGH0fEx4EXI+J64Fxg9nhuGhFXRcRJEfHaiPjLiNidLgw8JyJOTB9/W3T9NRHxmoiYFRE/Gs+9zSqhXtuYe68qq6c86zAG08ddkl4L/BKYUbUamTWBeg0Nea8qq6c8AeO6dGbTx0nyCS9Ln5u1rWndXfRnBIdaDA15ryqrl5JDUpIekXQlcHdEPBcRP42IV0fEMRHxzzWso1nD8dCQtaNyOYz/RNKbuEPS/ZI+nE53NWt7C+f0sGzRbHq6uxAwZXInkyZO4PKbNzJv+Tqvi7CWpDxnIUk6A7gQ+AtgK3BjRHylynUbl97e3ujr66t3NawNFGZMFSfBuzo7WLYomRvifIM1E0nrI6I3873RHJ4n6UzgsyQbB06qTPWqwwHDamXe8nWZ+Ywpkzv5w+D+zEDioGGNqlzAyLO9+eskXSvpSeBqkjUT/ttulio1M+q5lwbrMvXWrFrKbT74KZJhqOeAm4B5EbG9VhUzaxalZkyV4lXZ1qzK9TB2A29NN/P7jIOFWbZSM6a6uzozr/eqbGtW9dh80KyllFpMB2Qmwz311ppVnoV7ZjaCcovpPEvKWoUDhlkVeVW2tZJySe+55T4YEQ9WvjpmZtaoyvUw/iF9PBzoBR4iOf3u3wH3A39a3aqZmVkjKTlLKiLOioizgCeBuelsqdOBOSSrvc3MrI3kOQ/jpIjYVHgREQ8Dp1WvSmZm1ojyJL0flfRV4FtAAO8BHq1qrczMrOHkCRjvA/4G+FD6+h7gS1WrkZmZNaQRA0ZE/EHSl4EfRoQ3wTEza1N5Nh88D9gI3J6+Pk3SmmpXzMzMGkuepPdVwOuBXQARsRGf6W1m1nbyBIy9EfG7qtfEzMwaWp6k98OS/jPQIelE4IPA/6lutczMrNHk6WH8LXAKyXbn3wZ+x8EZU2Zm1iby9DDOjYgrgSsLBZIuAG6pWq3MzKzh5OlhLM1ZZmZmLazcbrVvBd4G9Ej6QtFbRwF7q10xMzNrLOWGpHYAfcB5wPqi8heAy6tZKTMzazzljmh9CHhI0neBFyNiH4CkDmBSjepnZmYNIk8O4w6g+NT6LuDH1amOmZk1qjwB4/CI+H3hRfp8cvWqZGZmjSjPtNoXJc0tHMkq6XRgoLrVMstn9YZ+Vqzdwo5dA0zr7mLx/Fk+Q9usSvIEjA8Dt0jakb4+DriwelUyy2f1hn6WrtrEwOA+APp3DbB0VXLWl4OGWeXl2d78/0o6CZhFcqb3YxExWPWamY1gxdotB4JFwcDgPlas3eKAYVYF5dZhnB0R6yQtGvLWiZKIiFVVrptZWTt2ZY+MFpd7yMqscsr1MP4DsA54R8Z7AThgWF1N6+6iPyNoTOtOJvXlGbJyQDHLr+QsqYi4Kn18X8bP+8dzU0ndkm6V9JikRyW9QdLRku6U9Hj6OKXo+qWStkraImn+eO5trWPx/Fl0dXYcUtbV2cHi+bOA8kNWcDCg9O8aIDgYUFZv6K9J/c2aTbkhqY+U+2BEXDuO+34euD0izpd0GMk03b8D7oqI5ZKWAEuAKySdDFxEsmPuNODHkv6ksJDQ2lehJ1CqhzDSkJVzIGajU25I6sj0cRbwOqBwLOs7gHvGekNJRwFvAt4LEBF7gD2SFgBnppddD/wEuAJYANwUEbuBbZK2kpwAeO9Y62CtY+GcnmHDS5ffvJFp3V10T+7kuZeGz88oDFnlyYGY2UHltga5GkDSHcDciHghff1Jxre1+auBncDXJJ1Ksk/Vh4BjI+KZ9N7PSDomvb4HuK/o89vTsmEkXQpcCnDCCSeMo4rWbLLyFZ0TRGeHGNwXB64rHrIaKQdiZofKs9L7BGBP0es9jO9M74nAXOBLETEHeJFk+KkUZZRFRhkRcV1E9EZE79SpU8dRRWs2WcNLg/uDIw6bSE93FwJ6urtYtmj2gR5JVg4E4MXde53HMMuQZ+HeN4EH0k0IA3gn8I1x3HM7sD0i7k9f30oSMH4l6bi0d3Ec8GzR9ccXfX46yU66ZgeUGkbaNTDIEZOy/5oXAsfV39t8yNDVroFBLwA0yzBiDyMirgHeBzwH7ALeFxGfGusNI+KXwNOSZqVF5wCPkORILk7LLgZuS5+vAS6SNEnSTOBE4IGx3t+a2+oN/cxbvo6ZS37AvOXrDvQESg0jCcrOglo4p4fJhw0PKMWzqcwskaeHAckspucj4muSpkqaGRHbxnHfvwVuSGdI/RtJQJoArJR0CfAUcAFARGyWtJIkqOwFLvMMqfZUbl3F4vmzDnkPkmAxdOwyaxaUk99m+YwYMCRdBfSSzJb6GtAJfAuYN9abRsTG9HcOdU6J668Brhnr/aw1lJsG+7MlZx+4pjDFNiuhDcMDgZPfZvnk6WG8E5gDPAgQETskHVn+I2aVN1JPoHiKLcC85etyBYKs3knxbCozS+SZJbUnIoK0dy/piOpWySxbqf/xlyofaSV4wcI5PSxbNLvkbCozS+TpYayU9M9At6S/Bt4PfKW61TIbbrQ9gZFWgg+91gHCrLyyAUOSgJuBk4DnSfIYn4iIO2tQN7NDjCYAFH/GgcCsMsoGjIgISasj4nTAQcKqJu+usQ4AZvWTJ4dxn6TXVb0m1ra8a6xZc8gTMM4iCRq/kPRzSZsk/bzaFbP2MdI25GbWGPIkvd9a9VpYU6rU4UOl1ksUl/ugI7P6K3cexjEkZ1T8MbAJWBYRz9eqYtbYslZdL77loQP7MnVI7IugZ8iXe9YXf+HaoTqkkvfyXk9mtafI+IcKIOl2kq3H7wHeDhwZEe+tXdXGp7e3N/r6+updjZZValFclq7ODpYtmg2QOS126HBUMQETSgSUnu6uAyu8zawyJK2PiKydOMoOSf1RRFyZPl8r6cHKV82a1Wj2WRoY3Mcn12zmiEkTM3MVpXoYkKwWLfWe93oyq61yAUPpudqF8yg6il9HxG+rXTlrHEOHkkqdZlfKroFBdg1kX78vYsSeRhbv9WRWW+UCxstJhqSKDzAq9DKC5OQ8awN5T7Mbq0KeoxCQ8vxG7/VkVnslp9VGxIyIeHVEzMz4cbBoI6VOs6tEsAB4ac9eLr95IwCfvfC0std6ryez+sl7Hoa1sWrnCgpDW8Wzn0rZtvzcqtbFzErLs3DP2lwtcwUDg/syD3EHOOKw4edvm1ntlFuH8UPgv0XEE7WrjtXb6g39h5xx3d3VydtPPY7vrO8fdVJ6rEoNdL20xwctmtVTuSGprwN3SLoe+PuIyD8lxhpe1gI6gMW3PnRIbmLXwCDfvu8pOidWpjNaODa1p7uLl/bszZxpVWqarWdFmdVXyYARESsl/QD4BNAn6ZvA/qL3r61B/awKSq2cnjRxQmYiez+we+/+YeUjEfDG1xzNE78ZyNzSY2g9IJn99Ben9wzr0XhWlFn9jZT0HgReBCYBR1IUMKx5ldrsr9JDTgHc92/PsT8ic/+ncudb9L7qaO8dZdZgyuUw3gJcC6wB5kbESzWrlVVVLVdIF4aWSu3/VOp8C597YdZ4yg1MXwlcEBFLHCxaS6lcwJTJnXR2lJqjNH7estysuZVbuPdnEbG5lpWx2lg8fxZdnYdOUe3q7OCqd5zCivNPZcrkzqrd2/s/mTUvL9xrQyOdjV08FLR6Qz8fTldhV4JnOpk1LweMNpU3R7BwTg99T/6WG+57KtceTwVTJnfyh8H9nulk1kK80ttG9D8XzuazF55GT3fXgb2cyg1bFYa3li2afchnvP+TWXMreYBSs/MBSsNV8pjTrDUUkKwM/+R5pzgwmDWpsR6gZC3kY6s3HTKsNN5jTkfKg5hZ63HAaFHFvYlShx0VprmO9UveayXM2osDRgsaOlxU7mQ8T3M1s7yc9G5BWVt/lOJprmaWl3sYLWg0vYazTpo6rGz1hn4+uWbzgTO4p0zu5Kp3OJFt1u7cw2gxqzf0M0H5t/f4zvp+Vm/oP+Tzi2956ECwgGRIa/GtDx1ynZm1HweMFlLIXWSdJdHZocyT7Ibu77Ri7RYG9w///OC+8D5QZm3OAaOFlMpddEisOP/Ukp8rHsIqN5zlBLlZe3PAaCGlvtALPY5SCe7i8nJJcCfIzdpb3QKGpA5JGyR9P319tKQ7JT2ePk4punappK2StkiaX686N7pyX+hLV23irJOmZu5SW7y/0+L5s+icMHzwqrND3gfKrM3Vs4fxIeDRotdLgLsi4kTgrvQ1kk4GLgJOAd4CfFFSBzZMqS97SHIVdz+2c8T9nRbO6WHFBafS3XVwr6gpkztZcf6pniVl1ubqMq1W0nTgXOAa4CNp8QLgzPT59cBPgCvS8psiYjewTdJW4PXAvTWsckMaujfUWSdNJTOzndqxayDX6myv4DazLPXqYXwO+CiHnhF+bEQ8A5A+HpOW9wBPF123PS0bRtKlkvok9e3cubPytW4ghRlR/bsGCJK9oW647ykG95XeTNI5CDMbj5oHDElvB56NiPV5P5JRlvmtGBHXRURvRPROnTp8QVoryZoRVW7fYYFzEGY2LvUYkpoHnCfpbcDhwFGSvgX8StJxEfGMpOOAZ9PrtwPHF31+OrCjpjVuIIVhqP5RTnENxrYrrZlZQc17GBGxNCKmR8QMkmT2uoh4D7AGuDi97GLgtvT5GuAiSZMkzQROBB6ocbUbQvEw1Gh1jGL1t5lZlkbaS2o5sFLSJcBTwAUAEbFZ0krgEWAvcFlE5NtZr8WMZlPBobJWf5uZjUZdA0ZE/IRkNhQR8RvgnBLXXUMyo6qtjaVnUdDjhLeZjZNXejeJ1Rv6y82YLcuL7sysEhwwmsSKtVvKzoIqy6NRZlYBDhhNYjwb/w3u906zZjZ+jZT0bitDV2kvnj+r7LTXad1d48pheKdZMxsv9zDqIGuV9tJVm8oeULR4/qxhGweOJqfhVd5mNl4OGHWQNT126EFGQy2c0zNs48B3n3HCsCDSOUF0dhwaSobuSGtmNhYekqqDUsND5YaNSg1h9b7q6GHlwKiGu8zM8nDAqINS+YhSw0aFIaxCr6QwhAWld5Z1gDCzSvOQVB1k5SPKDRuNZQjLzKzS3MOog8L//vMOG41lCMvMrNIcMOpkNIcUjXYIy8ysGjwkVSerN/Qzb/k6Zi75AfOWrxv1lFrPfDKzWnMPow5GSmIPNdohLDOzanDAqINySexSQcDnbJtZvXlIqg6cxDazZuSAUQelktVOYptZI3PAqAMnsc2sGTmHUQdOYptZM3LAqBMnsc2s2XhIyszMcnHAMDOzXBwwzMwsF+cwxmi0R6yamTU7B4wxGO3WHmZmrcBDUmPg8ynMrB05YIyBt/Yws3bkIakiefMSPp/CzNqRexipQl6if9cAwcG8RNY5Fd7aw8zakQNGajR5iYVzeli2aDY93V0I6OnuYtmi2U54m1lL85BUarR5CW/tYWbtxj2MlLccNzMrzwEj5byEmVl5HpJKectxM7PyHDCKOC9hZlaah6TMzCwXBwwzM8vFAcPMzHJxwDAzs1wcMMzMLBdFRL3rUBWSdgJPAq8Efl3n6tST2+/2u/3tayztf1VETM16o2UDRoGkvojorXc96sXtd/vdfre/Ur/PQ1JmZpaLA4aZmeXSDgHjunpXoM7c/vbm9re3ira/5XMYZmZWGe3QwzAzswpwwDAzs1yaOmBIOl7S3ZIelbRZ0ofS8qMl3Snp8fRxStFnlkraKmmLpPn1q33lSOqQtEHS99PXbdN+Sd2SbpX0WPr34A1t1v7L07/7D0u6UdLhrd5+Sf8q6VlJDxeVjbrNkk6XtCl97wuSVOu2jEWJ9q9I/w38XNJ3JXUXvVe59kdE0/4AxwFz0+dHAv8POBn4e2BJWr4E+HT6/GTgIWASMBP4BdBR73ZU4M/hI8C3ge+nr9um/cD1wH9Jnx8GdLdL+4EeYBvQlb5eCby31dsPvAmYCzxcVDbqNgMPAG8ABPwIeGu92zaO9v9HYGL6/NPVan9T9zAi4pmIeDB9/gLwKMk/ogUkXySkjwvT5wuAmyJid0RsA7YCr69trStL0nTgXOCrRcVt0X5JR5H84/kXgIjYExG7aJP2pyYCXZImApOBHbR4+yPiHuC3Q4pH1WZJxwFHRcS9kXx7fqPoMw0tq/0RcUdE7E1f3gdMT59XtP1NHTCKSZoBzAHuB46NiGcgCSrAMellPcDTRR/bnpY1s88BHwX2F5W1S/tfDewEvpYOyX1V0hG0Sfsjoh/4DPAU8Azwu4i4gzZp/xCjbXNP+nxoeSt4P0mPASrc/pYIGJJeBnwH+HBEPF/u0oyypp1XLOntwLMRsT7vRzLKmrb9JP+7ngt8KSLmAC+SDEeU0lLtT8fpF5AMNUwDjpD0nnIfyShr2vbnVKrNLflnIelKYC9wQ6Eo47Ixt7/pA4akTpJgcUNErEqLf5V2uUgfn03LtwPHF318OkkXvlnNA86T9ARwE3C2pG/RPu3fDmyPiPvT17eSBJB2af+fA9siYmdEDAKrgDfSPu0vNto2b+fgsE1xedOSdDHwduDd6TATVLj9TR0w0qz+vwCPRsS1RW+tAS5On18M3FZUfpGkSZJmAieSJH6aUkQsjYjpETEDuAhYFxHvoX3a/0vgaUmz0qJzgEdok/aTDEWdIWly+m/hHJI8Xru0v9io2pwOW70g6Yz0z+6vij7TdCS9BbgCOC8iXip6q7Ltr3fGf5yzBf6UpBv1c2Bj+vM24BXAXcDj6ePRRZ+5kmSmwBaaZFZEzj+LMzk4S6pt2g+cBvSlfwdWA1ParP1XA48BDwPfJJkN09LtB24kydkMkvxP+ZKxtBnoTf/cfgH8E+nOF43+U6L9W0lyFYXvwS9Xo/3eGsTMzHJp6iEpMzOrHQcMMzPLxQHDzMxyccAwM7NcHDDMzCwXBwxrO0r8b0lvLSp7l6Tbi17fL2mjpKck7Uyfb0y3oMlzj9MkvS2j/AhJv5H08iHlqyW9q8zv+32e+5pVk6fVWluS9FrgFpL9xzpI5q6/JSJ+MeS69wK9EfHfR/n7S35O0o3A7RFxffr65SRz4U+IQxddFX/m9xHxstHUwazS3MOwthQRDwPfI1kdexXwjaHBYihJr5F0u6T1kv6XpJPS8guUnEfxkKR7JB0G/A/gwrRXcuGQX3Ujycr8gncCtwMTJN0l6cH0nIIFGXU4U+m5J+nrf0qDU+F8g5+m9VtbtFXGByU9kp6VcNPo/qTMDppY7wqY1dHVwIPAHpJVryO5DvhARDwu6d8DXwTOBj4BzI+IfkndEbFH0ico3TO5HfiqpFdExG9Igsc/An8A3hkRz0t6JXCfpDWRYxgg3VPtH4EFEbEzDVLXkOxcugSYGRG7VXSwjtloOWBY24qIFyXdDPw+InaXuzbdEfmNwC1FB5NNSh9/Bnxd0kqSDQBHuu8eSWuA8yV9h2R7kztIdhD9lKQ3kWxX3wMcC/wyR3NmAa8F7kzr10GyfQQk26bcIGk1yfYpZmPigGHtbj+HniVSygRgV0ScNvSNiPhA2uM4F9goadg1GW4EPkYSJG6LiMF0aGkqcHr6+gng8CGf28uhQ8mF9wVsjog3ZNzrXJKDps4DPi7plDh42I5Zbs5hmOUQyTkr2yRdAAdmWp2aPjpdaHkAAADhSURBVH9NRNwfEZ8Afk2ynfQLJMcGl3I3yc6hl5EED4CXk5xvMijpLOBVGZ97Ejg53X305SQ71EKysdxUSW9I69Qp6RRJE4DjI+JukoO2ugEnz21MHDDM8ns3cImkh4DNJIcXAaxIk9QPA/eQnKF8N8kXe1bSm4jYT3KOyyvSz0By6E2vpL70Xo9lfO5pkrO7f55evyEt3wOcD3w6rd9GkiG0DuBbkjal1342kmNszUbN02rNzCwX9zDMzCwXBwwzM8vFAcPMzHJxwDAzs1wcMMzMLBcHDDMzy8UBw8zMcvn/ZVQmT0CZSs8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "plt.scatter(y_test, y_test_pred)\n",
    "\n",
    "plt.xlabel('Y Test Values')\n",
    "plt.ylabel('Y Predicted Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 4.9 ( 1 point) Compute r-squared value for the test set predictions. Hint: - google sklearn's score method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 is : 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print('R^2 is : %0.2f' % r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will perform polynomial regression and compare the performance with corresponding Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 4.10 (1 point) Obtain polynomial features on original X. It is important that we use original X and not the normalized X, to get polynomial features.\n",
    "\n",
    "Hint - Use PolynomialFeatures from sklearn.preprocessing. It will be efficient to use fit_transfom method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#define the model\n",
    "polyFeatures = PolynomialFeatures(2)\n",
    "\n",
    "#fit the X data and assign to and array \n",
    "polyFeatures = polyFeatures.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 4.11 (3 points) Normalize your features in the same manner as in Ques 4.3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "meanX = polyFeatures.mean(axis = 0)\n",
    "stdX = polyFeatures.std(axis = 0)\n",
    "\n",
    "X_p_norm = (polyFeatures - meanX)/stdX\n",
    "\n",
    "#check that the array has the correct properties\n",
    "#print(X_p_norm.var(axis = 0), X_p_norm.mean(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will remove the columns where standard deviation is zero. \n",
    "# As standard deviation is zero, which means the entire column has single value \n",
    "# Our algorithm will add the corresponding weight to the constant term\n",
    "# you don't have to enter any code in this cell. Just run the cell one time and it will delete the corresponding columns\n",
    "\n",
    "indDelete = np.where(stdX==0)[0]\n",
    "polyFeatures = np.delete(polyFeatures, indDelete, axis=1)\n",
    "meanX = np.delete(meanX,indDelete,axis=0)\n",
    "stdX = np.delete(stdX,indDelete,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00 -1.88610886e-15 -9.75573550e-17  1.62595592e-17\n",
      "  1.30076473e-16  3.25191183e-17  0.00000000e+00  2.92672065e-16\n",
      "  0.00000000e+00  3.25191183e-17 -1.13816914e-16  1.30076473e-16\n",
      " -1.62595592e-16  1.20320738e-15 -9.91833109e-16 -1.52839856e-15\n",
      "  4.84534863e-15  5.56076924e-15  0.00000000e+00 -5.28435673e-16\n",
      "  2.60152947e-16 -4.22748538e-16  6.50382367e-17  5.26809717e-15\n",
      " -1.95114710e-16  2.79257929e-15  6.50382367e-17  0.00000000e+00\n",
      " -6.50382367e-17  0.00000000e+00 -3.25191183e-17  6.50382367e-17\n",
      " -6.50382367e-17  6.50382367e-17 -3.25191183e-17 -3.25191183e-17\n",
      "  0.00000000e+00  0.00000000e+00  1.30076473e-16  0.00000000e+00\n",
      "  1.95114710e-16 -7.15420604e-16  1.04061179e-15  6.50382367e-16\n",
      " -1.95114710e-16  6.50382367e-17 -6.50382367e-16  3.57710302e-16\n",
      " -6.50382367e-17 -7.80458840e-16  6.50382367e-17  2.27633828e-16\n",
      "  9.10535314e-16  1.78855151e-16 -3.25191183e-17  0.00000000e+00\n",
      "  6.50382367e-17  0.00000000e+00  7.47939722e-16  0.00000000e+00\n",
      "  6.50382367e-17 -1.30076473e-16  6.50382367e-17 -1.46336033e-16\n",
      "  6.79649573e-15 -1.49587944e-15  2.79664418e-15  2.94298021e-15\n",
      "  1.49587944e-15 -4.22748538e-16 -7.96718399e-16  1.02435223e-15\n",
      "  4.22748538e-16  6.50382367e-17 -3.90229420e-16 -4.06488979e-16\n",
      "  2.52023167e-16  1.46336033e-16  4.87786775e-17 -3.25191183e-17\n",
      "  5.04046334e-16 -3.25191183e-17  6.48756411e-15  0.00000000e+00\n",
      " -2.43893388e-16 -6.50382367e-17 -1.62595592e-16 -1.78855151e-16\n",
      "  4.21122583e-15 -9.14600203e-16  1.56091768e-15  2.92672065e-15\n",
      "  1.88610886e-15 -5.52825012e-16 -7.96718399e-16 -1.21946694e-16\n",
      "  2.35763608e-16 -9.75573550e-17 -8.78016195e-16  2.43893388e-16\n",
      "  2.16658626e-15 -1.13816914e-16 -3.25191183e-17 -1.13816914e-16\n",
      " -1.13816914e-16 -1.13816914e-16  4.87786775e-17 -1.13816914e-16\n",
      " -9.75573550e-17  1.00809267e-15  6.30057918e-16  3.29256073e-16\n",
      "  9.10535314e-16 -3.67466037e-15 -6.50382367e-17 -6.01603689e-16\n",
      " -1.35767319e-15 -2.92672065e-16  3.25191183e-17 -1.20320738e-15\n",
      "  4.39008098e-16  2.30072762e-15  1.62595592e-16 -3.25191183e-17\n",
      " -3.25191183e-17 -3.25191183e-17  0.00000000e+00 -3.25191183e-17\n",
      "  1.30076473e-16  1.39832209e-15  2.14626181e-15  1.04061179e-15\n",
      "  1.00809267e-15  2.99175889e-15  6.50382367e-17  9.75573550e-17\n",
      "  3.25191183e-16 -1.62595592e-17 -6.50382367e-17 -1.56091768e-15\n",
      "  7.64199281e-16  3.12183536e-15  3.25191183e-17  0.00000000e+00\n",
      "  2.60152947e-16  0.00000000e+00 -1.30076473e-16  3.25191183e-17\n",
      "  0.00000000e+00 -6.50382367e-17  5.52825012e-16  3.57710302e-16\n",
      "  2.27633828e-16  1.30076473e-16 -4.22748538e-16  1.30076473e-16\n",
      " -9.75573550e-17 -6.50382367e-17 -3.25191183e-17  0.00000000e+00\n",
      "  6.50382367e-17  1.38206253e-16  5.85344130e-16  1.30076473e-16\n",
      "  0.00000000e+00 -1.13816914e-16 -3.25191183e-17 -3.25191183e-17\n",
      " -3.25191183e-17  0.00000000e+00 -3.25191183e-17 -1.30076473e-16\n",
      "  1.30076473e-16 -1.13816914e-16 -9.75573550e-17  6.50382367e-17\n",
      "  1.30076473e-16 -4.87786775e-17 -3.25191183e-17  0.00000000e+00\n",
      " -1.62595592e-16  0.00000000e+00 -6.50382367e-17  1.62595592e-16\n",
      " -2.27633828e-16 -3.57710302e-16 -2.60152947e-16 -2.60152947e-16\n",
      " -6.26806006e-15  3.25191183e-17  1.62595592e-16 -1.95114710e-16\n",
      "  3.25191183e-17 -1.95114710e-16  1.46336033e-16  6.50382367e-17\n",
      " -1.62595592e-16  0.00000000e+00  3.25191183e-17  1.95114710e-16\n",
      "  1.78855151e-16  6.50382367e-16  9.75573550e-17 -6.50382367e-17\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.50382367e-17 -3.25191183e-17 -6.50382367e-17  6.50382367e-17\n",
      "  8.12977959e-17 -1.78855151e-16  0.00000000e+00  0.00000000e+00\n",
      " -1.62595592e-16 -1.62595592e-17 -3.25191183e-17 -1.62595592e-17\n",
      "  6.50382367e-17 -4.39008098e-16  6.50382367e-17  0.00000000e+00\n",
      " -6.99161044e-16 -1.95114710e-16 -3.25191183e-16  8.04848179e-16\n",
      " -2.34950630e-15  3.25191183e-17 -1.62595592e-17 -7.96718399e-16\n",
      " -3.57710302e-16  4.06488979e-18 -4.55267657e-16  4.71527216e-16\n",
      "  2.39422009e-15  6.50382367e-17 -1.78855151e-16 -1.78855151e-16\n",
      " -8.94275754e-16 -3.57710302e-16 -1.11377980e-15 -1.47961988e-15\n",
      " -1.39832209e-15 -6.50382367e-17  1.04061179e-15 -1.46742522e-15\n",
      " -5.28435673e-16  4.87786775e-17  3.25191183e-16  2.60152947e-16\n",
      "  6.99161044e-16  3.25191183e-17 -1.62595592e-17 -4.21935560e-15\n",
      "  1.62595592e-16 -2.34137652e-15  1.05687135e-15 -4.09740891e-15\n",
      "  2.60152947e-16  5.52825012e-16 -2.11374269e-15 -6.50382367e-16\n",
      "  1.62595592e-17 -1.41458165e-15  1.30076473e-16  2.34137652e-15\n",
      " -3.25191183e-17  3.54458390e-15 -8.41432187e-16 -2.13813203e-15\n",
      "  0.00000000e+00  6.50382367e-16  2.92672065e-16 -9.10535314e-16\n",
      "  9.91833109e-16 -1.08939046e-15  6.50382367e-17 -4.97542511e-15\n",
      "  8.78016195e-16  4.30878318e-16  5.18679938e-15  9.75573550e-17\n",
      " -4.55267657e-16 -5.20305893e-16  6.50382367e-16 -1.13816914e-15\n",
      " -1.28450517e-15  7.31680163e-17  1.54465812e-15 -2.60152947e-16\n",
      "  6.50382367e-17  9.75573550e-17 -4.06488979e-17 -2.92672065e-16\n",
      " -5.04046334e-16  1.82920041e-15 -1.06500113e-15 -3.57710302e-16\n",
      "  3.25191183e-17  1.12190958e-15 -3.90229420e-16  2.96736955e-16\n",
      " -1.24385628e-15 -1.46336033e-16 -4.06488979e-17 -2.27633828e-16\n",
      " -4.87786775e-16 -6.50382367e-17  6.17863248e-16  1.47961988e-15\n",
      "  2.60152947e-16  1.78855151e-16 -2.96736955e-16  1.78855151e-15\n",
      " -2.92672065e-16 -5.81279240e-16 -8.45497077e-16 -5.13802070e-15\n",
      " -1.95114710e-16  1.13003936e-15 -1.13816914e-16  5.69084571e-16\n",
      "  1.62595592e-17 -6.82901485e-16  2.27633828e-16  2.15439159e-16\n",
      "  3.57710302e-16 -6.17863248e-16 -1.43084121e-15  2.35763608e-16\n",
      "  3.73969861e-16 -2.27633828e-16  1.62595592e-16  1.46336033e-16\n",
      " -5.44695232e-16 -5.52825012e-16  1.62595592e-16  6.50382367e-17\n",
      " -6.50382367e-17  8.45497077e-16  1.30076473e-16  3.25191183e-17\n",
      "  9.75573550e-17 -4.55267657e-16 -9.10535314e-16  7.31680163e-16\n",
      "  8.57691746e-16 -4.87786775e-16 -1.62595592e-17 -2.88607175e-16\n",
      " -6.58512146e-16  1.95114710e-16 -2.27633828e-16 -6.21928138e-16\n",
      "  1.62595592e-17 -8.94275754e-17 -1.95114710e-16  3.33320963e-16\n",
      " -4.87786775e-16 -1.86984930e-16 -5.36565453e-16  3.65840081e-17\n",
      " -7.15420604e-16 -1.30076473e-16 -6.99161044e-16  4.29252362e-15\n",
      " -3.90229420e-16 -3.90229420e-16 -8.61756636e-16  5.69084571e-17\n",
      " -1.32515407e-15] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "normalizedX = (polyFeatures - meanX)/stdX\n",
    "\n",
    "#check the variance and mean\n",
    "print(normalizedX.mean(axis = 0), normalizedX.var(axis =0))\n",
    "\n",
    "#no rows with nan are present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining polynomial features and standardizing the data we will split the data into training and test set and apply linear regression model to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 4.12 (2 points) Perform train-test split of normalized data and fit a linear regression model on top of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(normalizedX, Y, test_size = 0.2)\n",
    "reg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 4.13 (2 points) Get the prediction on training set and compute mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error on train data is: 4.47\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = reg.predict(X_train)\n",
    "\n",
    "print('Mean absolute error on train data is: %0.2f' % mean_absolute_error(y_train, y_train_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 4.14 (2 points) Get the prediction on test set and compute mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error on test data is: 144.62\n"
     ]
    }
   ],
   "source": [
    "y_test_predict = reg.predict(X_test)\n",
    "\n",
    "print('Mean absolute error on test data is: %0.2f' % mean_absolute_error(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 4.15 (1 point) Plot the test set predictions versus corresponding true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Predicted Y Value')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "plt.scatter(y_test, y_test_predict)\n",
    "\n",
    "plt.xlabel('Actual Y Value')\n",
    "plt.ylabel('Predicted Y Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows a classical case of overfitting. There are many possible reasons for this overfitting. A very simple reason can be less amount of training data as comapred to number of features present. Nevertheless, to prevent the overfitting, we use a technique called ridge regression. Under this technique we penalize the magnitude of weights that our model can have. This restricts the values of the coefficients and thus favouring simple models as compared to complex one. \n",
    "\n",
    "The loss function that we minimize is \n",
    "\n",
    "$||Y - w^TX||^2 +\\alpha\\sum_iw_i^2 $,\n",
    "\n",
    "where $\\alpha$ controls the model complexity. When $\\alpha$ equals 0, the above model corresponds to linear regression model.\n",
    "\n",
    "Under SKLearn, fitting a ridge regression model is very similar to the Linear regression model. We will use Ridge class under linear_model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 4.16 (2 points) Create a ridge regression model for $\\alpha$ equals 0.1 and fit on the training set of Ques 4.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "      normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dependecies\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "#define the model\n",
    "ridge_reg = Ridge(alpha = 0.1)\n",
    "\n",
    "ridge_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 4.17 (2 points) Get the prediction on training set and compute mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error on train data is: 19.11\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = ridge_reg.predict(X_train)\n",
    "\n",
    "print('Mean absolute error on train data is: %0.2f' % mean_absolute_error(y_train, y_train_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ques 4.18 (1 point) Get the prediction on test set and compute mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error on test data is: 25.38\n"
     ]
    }
   ],
   "source": [
    "y_test_predict = ridge_reg.predict(X_test)\n",
    "\n",
    "print('Mean absolute error on test data is: %0.2f' % mean_absolute_error(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prediction on test set has considerably improved by using polynomial features and ridge regression. \n",
    "\n",
    "###  Bonus Question ( 2 points)\n",
    "\n",
    "Try the fitting for 5 different $\\alpha$ values and report the best performing model i.e. $\\alpha$ for which mean absolute error on test set is minimum. Also report the corresponding mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MA for an alpha of 0.01 is: 23.78\n",
      "MA for an alpha of 0.1 is: 21.55\n",
      "MA for an alpha of 1.0 is: 20.17\n",
      "MA for an alpha of 10.0 is: 17.84\n",
      "MA for an alpha of 100.0 is: 25.38\n"
     ]
    }
   ],
   "source": [
    "#make a list of geometically spaced alphas\n",
    "alpha_values = np.geomspace(0.01, 100, num =5)\n",
    "\n",
    "for values in alpha_values:\n",
    "    #define model\n",
    "    ridge_reg = Ridge(alpha = values)\n",
    "    #fit data\n",
    "    ridge_reg.fit(X_train, y_train)\n",
    "    \n",
    "    #find predicted y-test values\n",
    "    y_test_predict = ridge_reg.predict(X_test)\n",
    "    \n",
    "    # print the resulting mean absolute error\n",
    "    print('MA for an alpha of ' + str(values) + ' is: %0.2f' % mean_absolute_error(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IJ Comments\n",
    "\n",
    "An $\\alpha$ of approximately 9 (calculations not fully shown) gives the best mean absolute error on the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A comment on overfitting. \n",
    "\n",
    "As you have noticed, we were working on a very limited dataset (437 material samples) and this limits the kind of models that we can fit on this data. When we generated polynomial features, we had about 373 columns. To get the most out of these 373 columns, we definately need more training samples. But in the situations when obtaining more data is not possible, you definately want to have maximum use of all the data available. In these situations, we employ a very popular method called cross-validation. Instead of splitting data into dedicated training and test, we divide the data into k folds or partitions.The k can be 2,3,4,5 depending on the computational resources avaialble. We train our model on all the partitions except one and measure the accuracy over left out set. We repeat this procedure for all the partitions and select the model based on avaerage performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse modeling refers to the set of models that result in most of the weights to be zero. Under these models, we can drop or delete the features where weights are zero resulting in compact input matrix. This method can also help in the situations with small data sets and a useful tool in feature selection. We only retain the features corresponding to non-zero weights.\n",
    "\n",
    "One way to achieve sparsity is through minimizing the following loss function\n",
    "\n",
    "$||Y - w^TX||^2 +\\alpha\\sum_i|w_i| $,\n",
    "\n",
    "where $\\alpha$ controls the model sparsity or the number of non zero weights. To fit this model, we can use SKLearn's Lasso library. Another efficient library for sparse models is glmnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeffiecients  [ 3.76342384e+01 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  1.01546332e+01  1.57067153e+01  2.09192206e+00\n",
      "  0.00000000e+00  0.00000000e+00 -2.12807013e+01  1.54185672e+01\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  1.09534272e+01  2.71987666e+01\n",
      "  0.00000000e+00  0.00000000e+00 -1.36196397e-01 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00  1.62237668e+01\n",
      "  0.00000000e+00  0.00000000e+00  1.17436729e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -1.15189772e+01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -2.22723907e+00  3.24950881e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00 -2.45086172e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  1.51492150e+01  0.00000000e+00  1.05762301e+01  0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      " -8.79278532e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  2.15716230e+00 -0.00000000e+00 -5.78831653e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  3.83630663e-01 -0.00000000e+00 -1.59111994e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -9.77492943e-01  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  4.05002766e-04  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -7.36635078e-01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.66824667e+01 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  4.02050874e+00  3.53996821e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  1.06104389e+01 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.78534200e+01  1.17187979e+01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  3.17289210e+00  0.00000000e+00 -0.00000000e+00 -6.40913100e+00\n",
      "  5.07857336e-01 -0.00000000e+00  0.00000000e+00  5.16731550e-02\n",
      " -0.00000000e+00 -1.41584183e+00 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  3.43702407e+00 -0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00  3.55409578e-01  6.42631575e-01  0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00  6.66504637e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      " -1.07614484e+01  0.00000000e+00 -1.30698364e-02  0.00000000e+00\n",
      "  2.33085261e+01 -0.00000000e+00 -1.22400417e+01 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00  8.92979468e-01\n",
      " -0.00000000e+00 -1.00006858e+01 -0.00000000e+00  0.00000000e+00\n",
      "  4.96316501e+00  0.00000000e+00 -0.00000000e+00  8.59365889e+00\n",
      " -1.20294290e+01 -0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  1.64626152e+01  1.51633589e+00  7.61488586e+00  6.39750246e+00\n",
      " -0.00000000e+00 -7.63933107e+01  0.00000000e+00 -0.00000000e+00\n",
      " -7.55942227e+00  3.31091818e+01  0.00000000e+00  0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      " -8.52560818e+00 -0.00000000e+00 -0.00000000e+00  5.99409080e+00\n",
      " -0.00000000e+00 -0.00000000e+00  4.70537703e+01  2.75902073e+01\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.21960520e+00  0.00000000e+00\n",
      "  1.10049450e+01 -0.00000000e+00  0.00000000e+00  6.77898329e+00\n",
      "  7.79225063e+00  1.24445127e+01  0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -1.83029363e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00  1.60392408e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -1.48304303e-01  1.72320364e+00\n",
      "  0.00000000e+00 -1.69163028e+01 -1.22854029e+01 -5.56830160e+00\n",
      " -0.00000000e+00  9.98703273e+00 -3.69392386e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.54095602e+01 -2.47377526e+01\n",
      " -0.00000000e+00  1.58737182e+01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  4.88567004e+00 -0.00000000e+00\n",
      " -8.53126151e+00 -1.40965437e+00 -5.99201762e+00  1.08718224e+01\n",
      "  2.55656242e+00  0.00000000e+00 -1.67249899e+01 -0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -2.33952115e+00  4.88418732e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -1.77648158e+00 -0.00000000e+00  0.00000000e+00 -1.90332522e+00\n",
      "  6.50965883e+00 -0.00000000e+00 -0.00000000e+00 -7.14083191e+00\n",
      "  8.11412286e-01 -2.82661047e+00 -4.48472272e+00  8.19533819e-01\n",
      " -0.00000000e+00 -0.00000000e+00 -4.49445329e+00 -0.00000000e+00\n",
      " -3.75205227e+00  4.59692454e+00 -1.26510834e+00  1.20349190e+00\n",
      "  0.00000000e+00 -2.55128648e+00  9.23874183e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -1.34432183e+00  0.00000000e+00 -3.69489009e+00\n",
      "  0.00000000e+00  0.00000000e+00  3.42689067e+00  0.00000000e+00\n",
      " -0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -4.49534505e-01  0.00000000e+00 -3.55792974e+00\n",
      " -0.00000000e+00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian Johnston\\Anaconda3\\envs\\MSE1065\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22700.516066385473, tolerance: 1198.3236567335246\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "#Code demonstration for fitting a sparse model \n",
    "from sklearn import linear_model\n",
    "sparseModel = linear_model.Lasso(alpha=0.1)\n",
    "sparseModel.fit(X_train,y_train)\n",
    "print(\"Coeffiecients \", sparseModel.coef_)\n",
    "\n",
    "#you can see most of the coefficients are zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
